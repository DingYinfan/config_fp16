[
    {
        "model_name": "codegemma-7b",
        "args": "--model=codegemma-7b --device=cuda --dataset=./llm_samples_new/codegemma/codegemma-7b-float16.json --dtype=float16 --max-model-len=4096 --save-output=./output/codegemma/codegemma-7b-float16.json --trust-remote-code"
    },

    {
        "model_name": "codellama_70b_w8a16_gptq",
        "args": "--model=codellama_70b_w8a16_gptq --device=cuda --dtype=float16 --quantization=gptq --tensor-parallel-size=2 --max-model-len=4096 --dataset=./llm_samples_new/codellama/codellama-70b-instruct-quant-float16-w8a16.json --save-output=./output/codellama/codellama-70b-instruct-quant-float16-w8a16.json"
    }
]
