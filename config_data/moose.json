[
    {
        "model_name": "moose_34b_server_w8a16_gptq",
        "args": "--vllm-path=moose_34b_server_w8a16_gptq --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --batch-size=4 --max-out-len=10 --work-dir=outputs/moose_34b_server_w8a16_gptq --tensor-parallel-size=2 --model-kwargs dtype=float16 quantization=gptq max_model_len=32768"
    }
]