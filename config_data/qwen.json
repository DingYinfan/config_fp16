[
    {
        "model_name": "Qwen-1_8B-Chat-bf16",
        "args": "--vllm-path=Qwen-1_8B-Chat --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Qwen-1_8B-Chat-bf16 --tensor-parallel-size=1 --model-kwargs dtype=bfloat16  max_model_len=2048"
    },
	
    {
        "model_name": "Qwen-7B-Chat-bf16",
        "args": "--vllm-path=Qwen-7B-Chat --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Qwen-7B-Chat-bf16 --tensor-parallel-size=1 --model-kwargs dtype=bfloat16"
    },
	
    {
        "model_name": "Qwen-14B-Chat",
        "args": "--vllm-path=Qwen-14B-Chat --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Qwen-14B-Chat --tensor-parallel-size=1 --model-kwargs dtype=float16"
    },
	
    {
        "model_name": "Qwen-14B-Chat-bf16",
        "args": "--vllm-path=Qwen-14B-Chat --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Qwen-14B-Chat-bf16 --tensor-parallel-size=1 --model-kwargs dtype=bfloat16"
    },
	
    {
        "model_name": "Qwen-72B-Chat",
        "args": "--vllm-path=Qwen-72B-Chat --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Qwen-72B-Chat --tensor-parallel-size=4 --model-kwargs dtype=float16 max_model_len=2048"
    },
	
    {
        "model_name": "Qwen-72B-Chat-bf16",
        "args": "--vllm-path=Qwen-72B-Chat --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Qwen-72B-Chat-bf16 --tensor-parallel-size=4 --model-kwargs dtype=bfloat16 max_model_len=2048"
    },
	
    {
        "model_name": "Qwen1.5-7B",
        "args": "--vllm-path=Qwen1.5-7B --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Qwen1.5-7B --tensor-parallel-size=1 --model-kwargs dtype=float16 max_model_len=16384"
    },
	
    {
        "model_name": "Qwen1.5-14B-Chat",
        "args": "--vllm-path=Qwen1.5-14B-Chat --device=cuda --datasets mmlu_gen_79e572 --data-dir=tinydata/mmlu  --work-dir=outputs/Qwen1.5-14B-Chat --tensor-parallel-size=1 --model-kwargs dtype=float16 max_model_len=8192"
    },
	
    {
        "model_name": "Qwen1.5-32B",
        "args": "--vllm-path=Qwen1.5-32B --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Qwen1.5-32B --tensor-parallel-size=2 --model-kwargs dtype=float16 max_model_len=2048"
    },
	
    {
        "model_name": "Qwen1.5-72B-Chat",
        "args": "--vllm-path=Qwen1.5-72B-Chat --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --tensor-parallel-size=4 --work-dir=outputs/Qwen1.5-72B-Chat --tensor-parallel-size=4 --model-kwargs dtype=float16  max_model_len=2048"
    },

    {
        "model_name": "Qwen1.5-32B-Chat-GPTQ-INT4",
        "args": "--vllm-path=Qwen1.5-32B-Chat-GPTQ-INT4 --device=cuda --datasets=triviaqa_gen --data-dir=tinydata/triviaqa --work-dir=outputs/Qwen1.5-32B-Chat-GPTQ-INT4 --tensor-parallel-size=1 --model-kwargs dtype=float16 max_model_len=32768 quantization=gptq"
    },

    {
        "model_name": "qwen1.5_32b_w4a16_gptq",
        "args": "--vllm-path=qwen1.5_32b_w4a16_gptq --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs/qwen1.5_32b_w4a16_gptq --model-kwargs dtype=float16  max_model_len=4096 quantization=gptq"
    },

    {
        "model_name": "qwen1.5-32b-w4a16c8",
        "args": "--vllm-path=qwen1.5_32b_w4a16_gptq --device=cuda --datasets=triviaqa_gen --data-dir=tinydata/triviaqa --work-dir=outputs/qwen1.5-32b-w4a16c8 --model-kwargs dtype=float16 max_model_len=32768 quantization=gptq"
    },

    {
        "model_name": "qwen1.5-4b-chat-bf16",
        "args": "--vllm-path=qwen1.5-4b-chat --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --batch-size=4 --max-out-len=10 --work-dir=outputs/qwen1.5-4b-chat-bf16 --tensor-parallel-size=1 --model-kwargs dtype=bfloat16 max_model_len=32768"
    },

    {
        "model_name": "qwen1.5-4b-bf16",
        "args": "--vllm-path=qwen1.5-4b --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --batch-size=4 --max-out-len=10 --work-dir=outputs/qwen1.5-4b-bf16 --tensor-parallel-size=1 --model-kwargs dtype=bfloat16 max_model_len=32768"
    },

    {
        "model_name": "Qwen1.5-MoE-A2.7B-bf16",
        "args": "--vllm-path=Qwen1.5-MoE-A2.7B --device=cuda --datasets=mmlu_gen --work-dir=outputs/Qwen1.5-MoE-A2.7B-bf16 --data-dir=tinydata/mmlu/ --model-kwargs dtype=bfloat16 max_model_len=4096"
    },

    {
        "model_name": "qwen2-0.5b-instruct-bf16",
        "args": "--vllm-path=qwen2-0.5b-instruct --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval/ --tensor-parallel-size=1 --work-dir=outputs/qwen2-0.5b-instruct-bf16 --model-kwargs dtype=bfloat16 gpu_memory_utilization=0.945 --batch-size=32 --max-out-len=100"
    },

    {
        "model_name": "Qwen2-1.5B-Instruct-bf16",
        "args": "--vllm-path=Qwen2-1.5B-Instruct --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --batch-size=4 --max-out-len=10 --work-dir=outputs/Qwen2-1.5B-Instruct-bf16 --tensor-parallel-size=1 --model-kwargs dtype=bfloat16 max_model_len=32768"
    },
	
    {
        "model_name": "qwen2-57b-a14b-bf16",
        "args": "--vllm-path=qwen2-57b-a14b --device cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs/qwen2-57b-a14b-bf16 --tensor-parallel-size=4 --max-out-len=10 --batch-size=1 --model-kwargs dtype=bfloat16 max_model_len=8192 gpu_memory_utilization=0.945"
    },

    {
        "model_name": "qwen2-72b-instruct-gptq-int4-triviaqa",
        "args": "--vllm-path=qwen2-72b-instruct-gptq-int4 --device=cuda --datasets=triviaqa_gen --data-dir=tinydata/triviaqa --work-dir=outputs/qwen2-72b-instruct-gptq-int4-triviaqa --tensor-parallel-size=2 --model-kwargs dtype=float16 max_model_len=32768 quantization=gptq"
    },
	
    {
        "model_name": "qwen2-72b-instruct-gptq-int4-ceval",
        "args": "--vllm-path=qwen2-72b-instruct-gptq-int4 --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval/ --work-dir=outputs/qwen2-72b-instruct-gptq-int4-ceval --tensor-parallel-size=4 --max-out-len 10 --model-kwargs dtype=float16 max_model_len=32768 quantization=gptq"
    },
	
    {
        "model_name": "Qwen2-7B",
        "args": "--vllm-path=Qwen2-7B --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval/ --tensor-parallel-size=1 --work-dir=outputs/Qwen2-7B --model-kwargs dtype=float16 max_model_len=32768 gpu_memory_utilization=0.945 --batch-size=16 --max-out-len=100"
    },
	
    {
        "model_name": "qwen2-7b-instruct-bf16",
        "args": "--vllm-path=qwen2-7b-instruct --device=cuda --datasets=ceval_gen --data-dir tinydata/ceval/formal_ceval/ --tensor-parallel-size=1 --work-dir=outputs/qwen2-7b-instruct-bf16 --model-kwargs dtype=bfloat16 --batch-size=16 --max-out-len=100"
    },

    {
        "model_name": "qwen_14b_chat_w8a16_gptq",
        "args": "--vllm-path=qwen_14b_chat_w8a16_gptq --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/qwen_14b_chat_w8a16_gptq --tensor-parallel-size=1 --model-kwargs dtype=float16 quantization=gptq"
    },
	
    {
        "model_name": "qwen1.5_14b_chat_w8a16_gptq",
        "args": "--vllm-path=qwen1.5_14b_chat_w8a16_gptq --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/qwen1.5_14b_chat_w8a16_gptq --tensor-parallel-size=1 --model-kwargs dtype=float16 max_model_len=2048 quantization=gptq"
    },

    {
        "model_name": "qwen_72b_chat_w8a16_gptq",
        "args": "--vllm-path=qwen_72b_chat_w8a16_gptq --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/qwen_72b_chat_w8a16_gptq --tensor-parallel-size=4 --model-kwargs dtype=float16 quantization=gptq max_model_len=2048"
    },

    {
        "model_name": "qwen1.5_32b_w8a16_gptq",
        "args": "--vllm-path=qwen1.5_32b_w8a16_gptq --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs/qwen1.5_32b_w8a16_gptq --model-kwargs max_model_len=4096 dtype=float16 quantization=gptq"
    }
]