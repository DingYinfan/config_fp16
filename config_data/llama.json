[
    {
        "model_name": "llama_65B_hf",
        "args": "--vllm-path=llama_65B_hf --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --work-dir=outputs/llama_65B_hf --tensor-parallel-size=4 --model-kwargs dtype=float16 max_model_len=2048"
    },
	
    {
        "model_name": "llama-2-70b-hf",
        "args": "--vllm-path=llama-2-70b-hf --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs/llama-2-70b-hf --tensor-parallel-size=4 --model-kwargs dtype=float16 gpu_memory_utilization=0.945"
    },

    {
        "model_name": "llama-2-13b-chat-gptq",
        "args": "--vllm-path=llama-2-13b-chat-gptq --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --tensor-parallel-size=1 --work-dir=outputs/llama-2-13b-chat-gptq --model-kwargs dtype=float16 max_model_len=4096 gpu_memory_utilization=0.945 quantization=gptq --generation-kwargs temperature=0 --batch-size=4 --max-out-len=100"
    },

    {
        "model_name": "llama2_7b_chat_w8a8",
        "args": "--vllm-path=Llama-2-7b-chat-hf --device=cuda --datasets=triviaqa_gen --data-dir=tinydata/triviaqa/ --tensor-parallel-size=1 --work-dir=outputs/llama2_7b_chat_w8a8 --batch-size=16 --max-out-len=10 --max-num-workers=1 --model-kwargs dtype=float16 gpu_memory_utilization=0.945 max_model_len=4096"
    },

    {
        "model_name": "llama2_70b_w4a16_gptq",
        "args": "--vllm-path=llama2_70b_w4a16_gptq --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --tensor-parallel-size=2 --work-dir=outputs/llama2_70b_w4a16_gptq --model-kwargs dtype=float16 max_model_len=2048 quantization=gptq --batch-size=4 --max-out-len=10"
    },

    {
        "model_name": "llama2_70b_w4a16_gptq_w4a16c8",
        "args": "--vllm-path=llama2_70b_w4a16_gptq --device=cuda --datasets=triviaqa_gen --data-dir=tinydata/triviaqa --work-dir=outputs/llama2_70b_w4a16_gptq_w4a16c8 --tensor-parallel-size=4 --batch-size=32 --model-kwargs dtype=float16 quantization=gptq"
    },


    {
        "model_name": "llama-2-7b-chat-gptq",
        "args": "--vllm-path=llama-2-7b-chat-gptq --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs/llama-2-7b-chat-gptq --tensor-parallel-size=1 --generation-kwargs temperature=0 --batch-size=4 --max-out-len=10 --model-kwargs dtype=float16 quantization=gptq gpu_memory_utilization=0.945 max_model_len=4096"
    },

    {
        "model_name": "llama-2-7b-chat-gptq-W4A16C8",
        "args": "--vllm-path=llama-2-7b-chat-gptq --device=cuda --datasets=triviaqa_gen --data-dir=tinydata/triviaqa --work-dir=outputs/llama-2-7b-chat-gptq-W4A16C8 --tensor-parallel-size=1 --batch-size=32 --model-kwargs dtype=float16 quantization=gptq"
    },
	
    {
        "model_name": "Meta-Llama-3-8B",
        "args": "--vllm-path=Meta-Llama-3-8B --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Meta-Llama-3-8B --tensor-parallel-size=1 --model-kwargs dtype=float16"
    },
	
    {
        "model_name": "Meta-Llama-3-70B",
        "args": "--vllm-path=Meta-Llama-3-70B --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/Meta-Llama-3-70B --tensor-parallel-size=4 --model-kwargs max_model_len=4096  dtype=float16"
    },
 
    {
        "model_name": "Meta-Llama-3.1-8B-Instruct-bf16",
        "args": "--vllm-path=Meta-Llama-3.1-8B-Instruct --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --work-dir=outputs/Meta-Llama-3.1-8B-Instruct --tensor-parallel-size=1 --max-out-len=10 --batch-size=4 --model-kwargs dtype=bfloat16 max_model_len=32768"
    },

    {
        "model_name": "Meta-Llama-3-70B_gptq_w4a16_wo_kv",
        "args": "--vllm-path=Meta-Llama-3-70B_gptq_w4a16_wo_kv --device=cuda --datasets=ceval_gen --data-dir=tinydata/ceval/formal_ceval --work-dir=outputs/Meta-Llama-3-70B_gptq_w4a16_wo_kv --tensor-parallel-size=2 --max-out-len=10 --model-kwargs dtype=float16 max_model_len=8192 quantization=gptq"
    },

    {
        "model_name": "llama2_7b_w8a16_gptq",
        "args": "--vllm-path=llama2_7b_w8a16_gptq --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs/llama2_7b_w8a16_gptq --tensor-parallel-size=1 --model-kwargs dtype=float16 quantization=gptq"
    },
	
    {
        "model_name": "llama2_7b_w8a16_gptq-2cards",
        "args": "--vllm-path=llama2_7b_w8a16_gptq --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs/llama2_7b_w8a16_gptq --tensor-parallel-size=2 --model-kwargs dtype=float16 quantization=gptq"
    },
	
    {
        "model_name": "llama2_13b_w8a16_gptq",
        "args": "--vllm-path=llama2_13b_w8a16_gptq --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs/llama2_13b_w8a16_gptq --tensor-parallel-size=1 --model-kwargs dtype=float16 quantization=gptq"
    },
	
    {
        "model_name": "llama2_70b_w8a16_gptq",
        "args": "--vllm-path=llama2_70b_w8a16_gptq --device=cuda --datasets=mmlu_gen --data-dir=tinydata/mmlu --work-dir=outputs/llama2_70b_w8a16_gptq --tensor-parallel-size=2 --model-kwargs dtype=float16 quantization=gptq"
    },
	
    {
        "model_name": "llama3_8b_w8a16_gptq",
        "args": "--vllm-path=llama3_8b_w8a16_gptq --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/llama3_8b_w8a16_gptq --tensor-parallel-size=1 --model-kwargs dtype=float16 quantization=gptq"
    },
	
    {
        "model_name": "llama_3_70b_w8a16_gptq",
        "args": "--vllm-path=llama_3_70b_w8a16_gptq --device=cuda --datasets=mmlu_gen_79e572 --data-dir=tinydata/mmlu --work-dir=outputs/llama_3_70b_w8a16_gptq --tensor-parallel-size=2 --model-kwargs max_model_len=4096 quantization=gptq dtype=half gpu_memory_utilization=0.945"
    }

]